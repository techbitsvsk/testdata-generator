# TPC-DS Query Complexity and Spark Kubernetes Resource Impact Summary (2.4TB Dataset)

**Current Date & Time:** Saturday, June 21, 2025 at 10:13:13 PM BST
**Location:** Glasgow, Scotland, United Kingdom

**Cluster Configuration Context:** 16 Executors, 12GB Memory (per container, includes overhead), 2 vCPUs (per executor). Data size: 2.4TB TPC-DS.

**Impact Scale:**
* **Low**: Minimal impact, easily handled.
* **Moderate**: Manageable, typical operations, might see some resource usage.
* **High**: Significant resource consumption, potential for bottlenecks if not tuned.
* **Very High**: Major resource consumption, likely to be a bottleneck, strong potential for spills/slowdowns.
* **Extremely High**: Critical resource consumption, almost guaranteed spills, major bottlenecks, requires significant tuning or architecture changes.

---

| Query ID   | Query Name                                      | Query Type                                                | Key Complexities                                                                                                                                                                                                                                                                                                                                                                                                                                                          | Memory Impact (Executor)                                                                                                                                                                                                       | Storage (Disk I/O) Impact                                                                                                                                                                                              | Data Shuffle Impact                                                                                                                                                                                                          | Network Impact                                                                                                                                                                    |
| :--------- | :---------------------------------------------- | :-------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **Q0** | Multi-channel Sales Analysis (with Rank Filter) | Multi-source Aggregation, Window Function (`RANK`), Multi-table Join, **Highly Selective Filter (`category_rank`)**, Granular Aggregation. | - `UNION ALL` across 3 fact tables (initial high data read).<br/>- `RANK()` window function: Requires shuffle & in-partition sort on `(i_category, d_year)` (potentially large partitions).<br/>- **CRITICAL: `WHERE r.category_rank <= 50` filter**: Drastically prunes rows *after* the window function, making subsequent steps much cheaper.<br/>- Multiple joins (5 total).<br/>- Granular `GROUP BY` (16 columns), but on a *filtered* dataset. | **High (at `ranked_items` stage), then Moderate**: `RANK()` demands significant memory for sorting within partitions. Post-filter, memory needs for joins/aggregations drop significantly due to reduced data volume. | **High (at `ranked_items` stage), then Moderate**: `RANK()` may cause shuffle spills. Once data is filtered by `category_rank`, I/O for subsequent shuffles and writes is much lower.                                                | **High (at `ranked_items` stage), then Moderate**: `RANK()`'s `PARTITION BY` causes large shuffle. After `category_rank` filter, shuffles for final joins/group by are greatly reduced.                                            | **High (at `ranked_items` stage), then Moderate**: Driven by `RANK()`'s large shuffle. Subsequent network traffic is significantly lower due to pruning. |
| **Q1** | Multi-channel Sales Analysis                    | Multi-source Aggregation, Complex Filtering, `HAVING`, `ORDER BY`/`LIMIT`. | - Large `UNION ALL` across three fact tables.<br/>- Multiple Joins (`all_sales` to `date_dim`, `item`).<br/>- Selective filters on `d_quarter_name` and `i_category`.<br/>- `GROUP BY` with moderate cardinality.<br/>- `HAVING COUNT(DISTINCT s.channel) = 3` (low cardinality for `channel`).<br/>- `ORDER BY`/`LIMIT` 100. | **Moderate to High**: Aggregation hash tables, join buffers. Filters help reduce data volume for aggregation.                                                                                                        | **Moderate**: Shuffles for `GROUP BY` and global `ORDER BY`.                                                                                                                      | **Moderate to High**: `GROUP BY` and global `ORDER BY` shuffles. Broadcastable dimensions (date, item) reduce join shuffles.                                                                         | **Moderate to High**: Data transfer during shuffles.                                                                                                        |
| **Q2** | Customer Loyalty and Return Behavior            | Multi-CTE Aggregation, Self-Join, Left Join, Complex Filtering. | - Nested Aggregations (`customer_sales`, `customer_returns`).<br/>- **Large Self-Join** (`cs1` to `cs2` on `customer_sk` and `d_year + 1`), which is fact-to-fact like.<br/>- `COUNT(DISTINCT purchase_count)` within aggregation.<br/>- Complex filters on aggregated profits and return rate. | **High**: Aggregation hash tables for CTEs. **Self-join requires large memory/sort buffers** for `customer_sales` CTE.                                                                                  | **Very High**: Self-join will almost certainly lead to extensive shuffle spills due to its size and nature.                                                                    | **Very High**: Large shuffles for `GROUP BY` in CTEs and the **massive self-join**.                                                                                       | **Very High**: Dominated by the substantial self-join shuffle.                                                                                              |
| **Q3** | Promotion Effectiveness Across Channels         | Multi-CTE, Aggregation, `UNION ALL`, Join.                | - Nested Aggregations (`promo_sales`, `all_channel_sales`).<br/>- `all_channel_sales` performs `UNION ALL` of aggregated revenues from all three fact tables (potentially very large intermediate).<br/>- Join of `promo_sales` with `all_channel_sales`. | **High**: Aggregations in both CTEs consume memory. Join between potentially large aggregated results also needs memory.                                                                                        | **Moderate**: Shuffles for aggregations and the final join.                                                                                                               | **High**: Multiple shuffles from CTE aggregations and the final join.                                                                                                     | **High**: Data transfer across multiple shuffle phases.                                                                                                     |
| **Q4** | Inventory and Sales Correlation                 | Nested Subquery (`IN`), CTEs, Aggregation, Join, Filtering. | - `top_selling_items` CTE: Small, filtered (`LIMIT 50`) result, ideal for broadcast.<br/>- `monthly_sales_inventory` CTE: Joins `store_sales` with `inventory` (large fact table) and `date_dim`. Filtered by `IN (SELECT i_item_sk FROM top_selling_items)` which significantly prunes data.<br/>- Multi-key join condition for `inventory`. | **Moderate**: Filters reduce data. The `inventory` join (a large fact table) requires memory for its operations. `top_selling_items` is very small and broadcastable.                                      | **Moderate**: Shuffles for `GROUP BY` and the `inventory` join.                                                                                                           | **Moderate**: Shuffles for `GROUP BY` and `inventory` join. The `IN` clause is highly efficient if `top_selling_items` is broadcasted.                                                                 | **Moderate**: Data transfer for joins and aggregations.                                                                                                     |
| **Q5** | Web Sales Funnel Analysis (Spark SQL Compliant) | Complex Joins, Aggregation, Date Functions.               | - `web_journey` CTE: Chained joins of large fact tables (`web_returns` to `web_sales`) and multiple dimension tables. This is a fact-to-fact-like join.<br/>- High-cardinality `GROUP BY` within `web_journey` with `MIN(d_sale.d_date)`.<br/>- `DATEDIFF` function. | **High**: Numerous large joins and high-cardinality `GROUP BY` in `web_journey` CTE will consume significant memory for hash tables/sort buffers.                                                           | **Very High**: Extensive shuffle and intermediate sort/hash spills from complex multi-table joins and the high-cardinality `GROUP BY`.                                         | **Very High**: Multiple shuffles for chained large table joins and high-cardinality `GROUP BY` in `web_journey`.                                                          | **Very High**: Significant network traffic due to numerous large shuffles.                                                                                  |
| **Q6** | Complex Customer Segmentation (RFM)             | Multi-channel `UNION ALL`, Nested Aggregation, Window Functions (`NTILE`). | - `union_sales` CTE: Large `UNION ALL` across all three fact sales tables.<br/>- `customer_rfm` CTE: Aggregates `union_sales` by `customer_sk` with `COUNT(DISTINCT date_sk)` (expensive).<br/>- **Global Window Functions (`NTILE`)**: Applied over the entire `customer_rfm` dataset, requiring a **full global sort** of this potentially massive intermediate result. | **Very High**: Aggregations with `COUNT(DISTINCT)` consume memory. **Global sort for NTILE requires entire dataset in memory (or extensive spilling)** for a single/few tasks. | **Extremely High**: **Massive shuffle spills for global sort** from `NTILE` and `GROUP BY` in `customer_rfm`.                                                      | **Extremely High**: Large shuffle for `GROUP BY` in `customer_rfm`. **Massive global shuffle for NTILE window function** to perform the full sort.                          | **Extremely High**: Heaviest network load due to global sort.                                                                                               |
| **Q7** | Store Performance by Manager and Demographics   | Multi-table Join, Aggregation.                            | - Multiple joins: `store_sales` (large fact) with `store`, `customer`, `household_demographics`, `date_dim`.<br/>- Selective filters on `d_year` and `s_state`.<br/>- `GROUP BY` with high-cardinality key (including demographics) and `COUNT(DISTINCT c_customer_sk)`. | **High**: Join buffers. `GROUP BY` with many distinct groups and `COUNT(DISTINCT)` needs large hash tables.                                                                    | **Moderate**: Shuffles for joins and `GROUP BY`.                                                                                                                   | **High**: Shuffles for multiple joins (if not broadcast) and the high-cardinality `GROUP BY`.                                                                             | **High**: Data transfer for joins and aggregations.                                                                                                         |
| **Q8** | Cross-Sell Opportunity Analysis                 | Nested Subquery (`IN`), Aggregation, Filtering.           | - `electronics_customers` CTE: `DISTINCT` on a large subset of customer IDs (derived from `store_sales`).<br/>- Semi-join (`ss.ss_customer_sk IN (SELECT ...)`): Will be optimized to a hash join or broadcast join.<br/>- `GROUP BY` with `COUNT(DISTINCT ss_ticket_number)`. | **High**: `DISTINCT` and semi-join build hash sets. `GROUP BY` with `COUNT(DISTINCT)` consumes significant memory for hash tables.                                            | **High**: Shuffles for `DISTINCT` in the CTE, the semi-join, and the final `GROUP BY`.                                                                                 | **High**: Shuffles for `DISTINCT` in CTE, the semi-join, and the final `GROUP BY`.                                                                                         | **High**: Network traffic due to multiple shuffle stages.                                                                                                   |
| **Q9** | Profitability of Returned Items                 | Multi-table Join, Aggregation.                            | - Standard joins of `store_returns` (large fact) with dimension tables (`item`, `reason`, `date_dim`).<br/>- Selective filter on `d_year`.<br/>- `GROUP BY` with moderate cardinality (`i_category`, `r_reason_desc`). | **Moderate**: Standard join and aggregation memory usage. Filters reduce data volume, keeping memory manageable.                                                              | **Moderate**: Shuffles for joins and `GROUP BY`.                                                                                                                   | **Moderate**: Shuffles for fact-to-dimension joins and `GROUP BY`.                                                                                                         | **Moderate**: Standard network load for shuffles.                                                                                                           |
| **Q10** | Lag/Lead Analysis of Sales                      | Aggregation, Window Functions (`LAG`, `LEAD`).            | - `monthly_sales` CTE: Aggregates `web_sales` by `d_year`, `d_moy` (standard aggregation).<br/>- **Global Window Function (`LAG`/`LEAD` without `PARTITION BY`)**: Requires a **full global sort** of the entire `monthly_sales` dataset. This data must fit in memory of the processing task(s) or spill. | **High**: Aggregation memory. **Global window function requires entire aggregated data to be sorted in memory (or spill extensively)** for a single or very few tasks. | **Very High**: **Massive shuffle spills for global sort** for window function.                                                                                | **Very High**: Shuffle for `GROUP BY` in `monthly_sales`. **Very large global shuffle for window function** to perform the full sort.                                       | **Very High**: Significant network traffic from global sort.                                                                                               |
| **Q11** | High-Value, Low-Frequency Customers             | Nested Subquery (`SELECT AVG(...)`), Aggregation, Filtering. | - `customer_purchase_summary` CTE: Aggregates `store_sales` by `c_customer_sk` with `COUNT(DISTINCT ss_ticket_number)`.<br/>- Scalar subquery for `AVG(avg_spent) * 5` (efficiently computed).<br/>- Filters applied on aggregated results. | **High**: `customer_purchase_summary` CTE's aggregation with `COUNT(DISTINCT)` is memory-intensive.                                                                           | **High**: Shuffles for the `customer_purchase_summary` `GROUP BY`.                                                                                                        | **High**: Large shuffle for the `customer_purchase_summary` aggregation.                                                                                                   | **High**: Network traffic from aggregation shuffle.                                                                                                         |
| **Q12** | Catalog vs. Web Sales for Specific Demographics | Nested Subquery (`IN`), `UNION ALL`, Aggregation.         | - `high_value_customers` CTE: Selective joins on dimension tables for customer filtering (likely small, broadcastable).<br/>- `UNION ALL` combines two branches (catalog sales, web sales).<br/>- Each branch performs a semi-join (`IN`) to `high_value_customers` and then `GROUP BY i_category`. | **Moderate**: `IN` subquery (likely broadcasted) and two separate aggregations.                                                                                                | **Moderate**: Shuffles for the two independent aggregations.                                                                                                           | **Moderate**: Shuffles for semi-joins (if not broadcast) and two aggregations.                                                                                             | **Moderate**: Data transfer for semi-joins and aggregations.                                                                                               |
| **Q13** | Store Sales Seasonality                         | Aggregation, Window Function (`AVG(...) OVER (PARTITION BY ...)`). | - Standard joins of `store_sales` with `date_dim` and `store`.<br/>- Selective filter on `d_year`.<br/>- `GROUP BY s_store_type`, `d_moy` (low cardinality).<br/>- Partitioned Window Function (`AVG`) with very small partitions (max 12 rows per partition). | **Moderate**: Aggregation memory is manageable due to filters and low-cardinality grouping. Window function partitions are very small, so minimal memory impact.           | **Moderate**: Shuffles for `GROUP BY` and the partitioned window.                                                                                                         | **Moderate**: Shuffle for `GROUP BY`. Smaller shuffle for partitioned window.                                                                                               | **Moderate**: Network traffic from shuffles.                                                                                                               |
| **Q14** | Repurchasing Behavior After Returns (Spark SQL Compliant) | Complex Self-Join logic, Aggregation, Date Functions.     | - `returns_and_sales` CTE: **Complex Fact-to-Fact Join** (`store_returns` to `store_sales`) on two keys (`customer_sk`, `item_sk`).<br/>- High-cardinality `GROUP BY` within CTE with `MIN(d_sale.d_date)`.<br/>- `DATEDIFF` function. | **Very High**: **Fact-to-fact join will require massive hash tables/sort buffers**. High-cardinality `GROUP BY` is also memory intensive.                                     | **Extremely High**: **Massive shuffle and spill files** from fact-to-fact join and high-cardinality `GROUP BY` operation.                                                 | **Extremely High**: **Massive shuffles for the fact-to-fact join and the `GROUP BY`**.                                                                                    | **Extremely High**: Heaviest network load due to fact-to-fact join and large aggregation.                                                                  |
| **Q15** | Impact of Web Page Type on Sales                | Aggregation, `CUBE`.                                      | - Multi-table Joins (`web_sales` to `web_page`, `item`, `date_dim`).<br/>- Selective filter on `d_year`.<br/>- **`GROUP BY CUBE (wp_type, i_category)`**: Generates 4 grouping sets, requiring significant memory for internal structures and multiple aggregation levels. | **High**: Join buffers. `CUBE` needs significant memory for internal data structures to manage multiple aggregation levels concurrently.                                       | **High**: Shuffles for joins and `CUBE` aggregation. Likely spills for `CUBE` intermediate data.                                                                          | **High**: Shuffles for joins and **large shuffles for CUBE aggregation**.                                                                                                 | **High**: Significant network traffic from complex aggregations.                                                                                             |
| **Q16** | Geographic Sales and Profit Analysis            | Aggregation, `ROLLUP`.                                    | - Multi-table Joins (`store_sales` to `customer_address`, `date_dim`).<br/>- Selective filter on `d_year`.<br/>- **`GROUP BY ROLLUP (ca_country, ca_state, ca_city)`**: Generates hierarchical aggregation levels, which can be numerous if `ca_city` has high cardinality. | **High**: Join buffers. `ROLLUP` needs significant memory for internal data structures to manage hierarchical aggregation levels.                                              | **High**: Shuffles for joins and `ROLLUP` aggregation. Potential spills if `ca_city` has very high cardinality.                                                           | **High**: Shuffles for joins and **large shuffles for ROLLUP aggregation**.                                                                                               | **High**: Significant network traffic from complex aggregations.                                                                                             |
| **Q17** | Call Center Interaction and Sales Conversion    | Nested CTE, Join, Aggregation, Filtering.                 | - `call_center_interactions` CTE: Joins `catalog_returns` to `call_center`.<br/>- **Complex Fact-to-Fact Join** (`call_center_interactions` to `catalog_sales` on `cr_order_number`).<br/>- `GROUP BY` with multiple `COUNT(DISTINCT)` aggregates. | **Very High**: **Fact-to-fact join will consume massive memory for hash tables/sort buffers**. Multiple `COUNT(DISTINCT)` aggregates in `GROUP BY` are very memory intensive. | **Extremely High**: **Massive shuffle and spill files** from fact-to-fact join and `COUNT(DISTINCT)` aggregations.                                                          | **Extremely High**: **Massive shuffles for the fact-to-fact join and the `GROUP BY` with `COUNT(DISTINCT)`**.                                                              | **Extremely High**: Heaviest network load due to fact-to-fact join and large aggregation.                                                                  |
| **Q18** | Manufacturer-Centric Sales Performance          | `LEFT JOIN` (Fanout/Explode), Aggregation.                | - **Multiple `LEFT JOIN`s of `item` (dimension) to three large fact tables (`store_sales`, `catalog_sales`, `web_sales`)**.<br/>- **Potential for extreme fanout**: If not optimized (e.g., aggregate-before-join), this can create an enormous intermediate result set. | **Very High**: If fanout occurs, it creates an **enormous intermediate dataset**, requiring massive memory for joins and subsequent aggregation.                               | **Very High**: **Massive shuffle spills** if fanout is not prevented by optimizer or pre-aggregation.                                                                      | **Very High**: Large shuffles for multiple `LEFT JOIN`s and a potentially **enormous shuffle for the final GROUP BY**.                                                        | **Very High**: Significant network traffic due to potential fanout and large aggregation.                                                                   |
| **Q19** | Time-to-Ship Analysis                           | Simple Joins, Aggregation, Arithmetic.                    | - Standard joins of `catalog_sales` (large fact) with dimension tables (`warehouse`, `ship_mode`, `date_dim`).<br/>- Selective filter on `d_year`.<br/>- Simple arithmetic for `avg_days_to_ship`.<br/>- `GROUP BY` on low-cardinality keys. | **Moderate**: Standard join and aggregation memory usage. Filters reduce data, keeping memory manageable.                                                                      | **Moderate**: Shuffles for joins and `GROUP BY`.                                                                                                                   | **Moderate**: Shuffles for fact-to-dimension joins and `GROUP BY`.                                                                                                         | **Moderate**: Standard network load for shuffles.                                                                                                           |
| **Q20** | Year-over-Year Growth for Top Customers         | Nested Aggregation, Window Function (`LAG`), Subquery (`IN`), Filtering. | - `yearly_customer_sales` CTE: Aggregates `web_sales` by `customer_sk` and `d_year` (high-cardinality `GROUP BY`).<br/>- `top_customers` CTE: Global `ORDER BY`/`LIMIT 100` on aggregated sales (requires shuffle).<br/>- Semi-join (`IN`) to `top_customers` (highly efficient if broadcasted).<br/>- Partitioned Window Function (`LAG`) on limited data, with small partitions. | **High**: `yearly_customer_sales` aggregation is memory intensive. Top-N global sort for `top_customers` can be large. Partitioned `LAG` is efficient on limited data. | **High**: Shuffles for `yearly_customer_sales` `GROUP BY` and the `ORDER BY` for `top_customers`.                                                                           | **High**: Large shuffles for `yearly_customer_sales` aggregation and top-N global sort. Smaller shuffle for partitioned window on reduced data.                               | **High**: Network traffic from aggregations and top-N sort.                                                                                                 |
| **Q_CUBE** | Combinatorial Analysis of Profitability (GROUP BY CUBE) | Multi-table Join, **Extensive Aggregation (`GROUP BY CUBE`)**, Complex Aggregations, `HAVING`, `ORDER BY`. | - Joins fact with dims (`d.d_year = 2001` helps filter initial data).<br/>- **`GROUP BY CUBE (4 columns)`**: Generates $2^4 = 16$ grouping sets (all combinations), leading to a massive number of aggregated output rows.<br/>- **`COUNT(DISTINCT sb.ss_ticket_number)`**: Applied across all 16 grouping sets, `ss_ticket_number` is very high-cardinality, making this extremely expensive.<br/>- `HAVING` clause filters *after* full aggregation.<br/>- Global `ORDER BY` of massive result set. | **Extremely High**: `CUBE` with 16 grouping sets and `COUNT(DISTINCT)` demands enormous memory for hash tables/aggregates. Almost guaranteed extensive memory spills to local disk. High GC pressure.                   | **Extremely High**: **Massive shuffle spills** are almost guaranteed due to memory limits, requiring high disk write/read activity on executor local storage. Numerous temporary files for intermediate sorts/hashes.     | **Extremely High**: `CUBE` aggregation necessitates massive shuffles (for all 16 grouping sets). `COUNT(DISTINCT)` adds further shuffles. Final global `ORDER BY` requires another very large shuffle. | **Extremely High**: Dominated by the immense data shuffle volume. Extreme pressure on network bandwidth. Network latency becomes a major bottleneck.           |